{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivBUMJHDYZDr"
      },
      "source": [
        "# Systemy uczące się - Zad. dom. 1: Minimalizacja ryzyka empirycznego\n",
        "Celem zadania jest zaimplementowanie własnego drzewa decyzyjnego wykorzystującego idee minimalizacji ryzyka empirycznego.\n",
        "\n",
        "### Autor rozwiązania\n",
        "Uzupełnij poniższe informacje umieszczając swoje imię i nazwisko oraz numer indeksu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "NAME = \"Wojciech Kot\"\n",
        "ID = \"151879\""
      ],
      "metadata": {
        "id": "BC9xRtfNYZDt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpVqi8ybYZDt"
      },
      "source": [
        "## Twoja implementacja\n",
        "\n",
        "Twoim celem jest uzupełnić poniższą klasę `TreeNode` tak by po wywołaniu `TreeNode.fit` tworzone było drzewo decyzyjne minimalizujące ryzyko empiryczne. Drzewo powinno wspierać problem klasyfikacji wieloklasowej (jak w przykładzie poniżej). Zaimplementowany algorytm nie musi (ale może) być analogiczny do zaprezentowanego na zajęciach algorytmu dla klasyfikacji. Wszelkie przejawy inwencji twórczej wskazane. **Pozostaw komenatrze w kodzie, które wyjaśniają Twoje rozwiązanie.**\n",
        "\n",
        "Schemat oceniania:\n",
        "- wynik na zbiorze Iris (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 10%: +40%,\n",
        "- wynik na ukrytym zbiorze testowym 1 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 15%: +30%,\n",
        "- wynik na ukrytym zbiorze testowym 2 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 5%: +30%.\n",
        "\n",
        "Niedozwolone jest korzystanie z zewnętrznych bibliotek do tworzenia drzewa decyzyjnego (np. scikit-learn).\n",
        "Możesz jedynie korzystać z biblioteki numpy.\n",
        "\n",
        "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod), o ile będzie się w nim na koniec znajdowała kompletna implementacja klasy `TreeNode` w jednej komórce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8mKzKqj3YZDu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TreeNode:\n",
        "\tdef __init__(self, min_samples_split=30, alpha=0.05):\n",
        "\t\tself.left: TreeNode | None = None  # wierzchołek znajdujący się po lewej stornie\n",
        "\t\tself.right: TreeNode | None = None  # wierzchołek znajdujący się po prawej stornie\n",
        "\t\tself.value = None  # wartość liścia i próg na węźle niebędącym liście\n",
        "\t\tself.arg = None # argument po którym następuje podział na danym węźle\n",
        "\t\t# zmienne odpowiadające za prunning, aby zapobiec przeuczeniu drzewa\n",
        "\t\tself.min_samples_split = min_samples_split\n",
        "\t\tself.alpha = alpha\n",
        "\n",
        "\tdef entropy(self, y: np.ndarray) -> float:\n",
        "\t\t# Oblicza entropię zbioru docelowego y.\n",
        "\t\tunique, counts = np.unique(y, return_counts=True)\n",
        "\t\tp = counts / len(y)\n",
        "\t\treturn -np.sum(p * np.log2(p))\n",
        "\n",
        "\tdef best_split(self, data: np.ndarray, target: np.ndarray):\n",
        "\t\t# Oblicza najlepszy podział, na podstawie entropii\n",
        "\t\tn_samples, n_features = data.shape\n",
        "\t\t# zmienne best_ -> odpowiadają za finalnie zwracane wyniki, więc będą nadpisywane gdy znaleziony zostanie lepszy podział\n",
        "\t\tbest_gain = 0\n",
        "\t\tbest_split_value = None\n",
        "\t\tbest_split_feature = None\n",
        "\t\tbest_left_idx, best_right_idx = None, None # Indeksy do podziału na lewe i prawe poddrzewo\n",
        "\n",
        "\t\tbase_entropy = self.entropy(target) # Obliczenie Entropii przed podziałem\n",
        "\n",
        "\t\tfor feature in range(n_features):\n",
        "\t\t\t\tthresholds = np.unique(data[:, feature]) # potencjalne progi podziału (unikalne wartości)\n",
        "\t\t\t\tfor threshold in thresholds:  \t\t# iteracja po znalezionych progach\n",
        "\t\t\t\t\t\t# indeksy wartości większych i niewiększych od progu\n",
        "\t\t\t\t\t\tleft_idx = data[:, feature] <= threshold\n",
        "\t\t\t\t\t\tright_idx = ~left_idx\n",
        "\n",
        "\t\t\t\t\t\t# warunek pominięcia podziału, dla zmiennem min_samples_split, aby nie przeuczać\n",
        "\t\t\t\t\t\tif np.sum(left_idx) < self.min_samples_split or np.sum(right_idx) < self.min_samples_split:\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\t# obliczanie 'zysku' entropii, czyli różnicy pomiędzy tym co było przedpodziałem a średniej ważonej entropii poddrzew po podziale\n",
        "\t\t\t\t\t\tleft_entropy = self.entropy(target[left_idx])\n",
        "\t\t\t\t\t\tright_entropy = self.entropy(target[right_idx])\n",
        "\t\t\t\t\t\tweighted_entropy = (\n",
        "\t\t\t\t\t\t\t\t(np.sum(left_idx) / n_samples) * left_entropy +\n",
        "\t\t\t\t\t\t\t\t(np.sum(right_idx) / n_samples) * right_entropy\n",
        "\t\t\t\t\t\t)\n",
        "\t\t\t\t\t\tgain = base_entropy - weighted_entropy\n",
        "\n",
        "\t\t\t\t\t\t# no i jeśli znaleziono lepszy gain, to trzeba podpisać nowe zmienne, aby znaleźć najlepszy podział\n",
        "\t\t\t\t\t\tif gain > best_gain:\n",
        "\t\t\t\t\t\t\t\tbest_gain = gain\n",
        "\t\t\t\t\t\t\t\tbest_split_value = threshold\n",
        "\t\t\t\t\t\t\t\tbest_split_feature = feature\n",
        "\t\t\t\t\t\t\t\tbest_left_idx, best_right_idx = left_idx, right_idx\n",
        "\n",
        "\t\treturn best_split_feature, best_split_value, best_left_idx, best_right_idx, best_gain\n",
        "\n",
        "\tdef fit(self, data: np.ndarray, target: np.ndarray) -> None:\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tdata (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
        "\t\t\ttarget (np.ndarray): wektor klas o długości n, gdzie n to liczba przykładów\n",
        "\t\t\tBuduje drzewo na podstawie danych uczących\n",
        "\t\t\"\"\"\n",
        "\t\tfeature, value, left_idx, right_idx, gain = self.best_split(data, target)\n",
        "\t\tif gain > self.alpha:\n",
        "\t\t\t\t# przypisanie po czym i na jakiej wartości nastąpił podział\n",
        "\t\t\t\tself.arg = feature\n",
        "\t\t\t\tself.value = value\n",
        "\t\t\t\t# budowa Lewego i prawego poddrzewa rekurencyjnie\n",
        "\t\t\t\tself.left = TreeNode()\n",
        "\t\t\t\tself.right = TreeNode()\n",
        "\t\t\t\tself.left.fit(data[left_idx], target[left_idx])\n",
        "\t\t\t\tself.right.fit(data[right_idx], target[right_idx])\n",
        "\t\telse: #no a jeśli nie ma poprawy po splicie, to liść.\n",
        "\t\t\t\tself.value = np.bincount(target).argmax()  # Przypisujemy najczęstszą klasę do liścia\n",
        "\n",
        "\n",
        "\tdef pred_rec(self, data: np.ndarray):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tdata (np.ndarray): wektor cech danego przykładu\n",
        "\t\t\"\"\"\n",
        "\t\tif self.left is None and self.right is None:\n",
        "\t\t\treturn self.value # if liść -> return\n",
        "\t\telse:\n",
        "\t\t\t#else sprawdź czy w prawo czy w lewo i tam idź\n",
        "\t\t\tif data[self.arg] <= self.value:\n",
        "\t\t\t\treturn self.left.pred_rec(data)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn self.right.pred_rec(data)\n",
        "\n",
        "\tdef predict(self, data: np.ndarray) -> np.ndarray:\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tdata (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tnp.ndarray: wektor przewidzoanych klas o długości n, gdzie n to liczba przykładów\n",
        "\t\t\"\"\"\n",
        "\t\ty_pred = np.zeros(data.shape[0])\n",
        "\t\t# dla każdego wektora danych uruchom pred_rec, które znajdzie dla niego klasę decyzyjną\n",
        "\t\tfor i in range(data.shape[0]):\n",
        "\t\t\t\ty_pred[i] = self.pred_rec(data[i])\n",
        "\t\treturn y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojjPihHUYZDu"
      },
      "source": [
        "## Przykład trenowanie i testowania drzewa\n",
        "\n",
        "Później znajduje się przykład trenowania i testowania drzewa na zbiorze danych `iris`, który zawierający 150 próbek irysów, z czego każda próbka zawiera 4 atrybuty: długość i szerokość płatków oraz długość i szerokość działki kielicha. Każda próbka należy do jednej z trzech klas: `setosa`, `versicolor` lub `virginica`, które są zakodowane jak int.\n",
        "\n",
        "Możesz go wykorzystać do testowania swojej implementacji. Możesz też zaimplementować własne testy lub użyć innych zbiorów danych, np. innych [zbiorów danych z scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIwpss_RYZDv",
        "outputId": "880b07b0-9774-4a16-ad75-d3112157bd41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.88\n",
            "0.8813559322033898\n",
            "0.7508417508417509\n",
            "0.9042553191489362\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris, load_wine, load_digits, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\"\"\"\n",
        "best for now: 30, 0.05 z wynikami:\n",
        "0.88\n",
        "0.8813559322033898\n",
        "0.7508417508417509\n",
        "0.9042553191489362\n",
        "przy czym alpha miała w większości *znikomy* wpływ na wynik\n",
        "\"\"\"\n",
        "# te parametry można sobie tuningować, ale zakładam że jak zgłaszam zadanie, no to potrzebuję mieć te wartości już zadane...\n",
        "mss = 30\n",
        "a = 0.05\n",
        "\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
        "\n",
        "tree_model = TreeNode(min_samples_split=mss, alpha=a)\n",
        "tree_model.fit(X_train, y_train)\n",
        "y_pred = tree_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "# a tu jest ten kod co był, skopiowany 4 razy z innymi datasetami, bo robienie pętli trwałoby dłużej, a student z natury jest leniwy\n",
        "data = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
        "\n",
        "tree_model = TreeNode(min_samples_split=mss, alpha=a)\n",
        "tree_model.fit(X_train, y_train)\n",
        "y_pred = tree_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "data = load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
        "\n",
        "tree_model = TreeNode(min_samples_split=mss, alpha=a)\n",
        "tree_model.fit(X_train, y_train)\n",
        "y_pred = tree_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
        "\n",
        "tree_model = TreeNode(min_samples_split=mss, alpha=a)\n",
        "tree_model.fit(X_train, y_train)\n",
        "y_pred = tree_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}